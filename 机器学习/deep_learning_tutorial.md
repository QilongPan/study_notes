# 深度学习笔记

[转载至深度学习系列教程](<https://www.zybuluo.com/hanbingtao/note/433855>)

## 感知器(神经元)

神经元和感知器本质上是一样的，只不过我们说感知器的时候，它的激活函数是阶跃函数；而当我们说神经元时，激活函数往往选择为sigmoid函数或者tanh函数等。

阶跃函数：
$$
f(z)=\left\{\begin{matrix}
1 &z>0 \\ 
0 & otherwise
\end{matrix}\right.
$$
神经网络示意图：

![](.\image\神经网络示意图.png)

上图中每个圆圈都是一个神经元，每条线表示神经元之间的连接。我们可以看到，上面的神经元被分成了多层，层与层之间的神经元有连接，而层内之间的神经元没有连接。最左边的层叫做**输入层**，这层负责接收输入数据；最右边的层叫**输出层**，我们可以从这层获取神经网络输出数据。输入层和输出层之间的层叫做**隐藏层**。

隐藏层比较多（大于2）的神经网络叫做深度神经网络。而深度学习，就是使用深层架构（比如，深度神经网络）的机器学习方法。

那么深层网络和浅层网络相比有什么优势呢？简单来说深层网络能够表达力更强。事实上，一个仅有一个隐藏层的神经网络就能拟合任何一个函数，但是它需要很多很多的神经元。而深层网络用少得多的神经元就能拟合同样的函数。也就是为了拟合一个函数，要么使用一个浅而宽的网络，要么使用一个深而窄的网络。而后者往往更节约资源。

深层网络也有劣势，就是它不太容易训练。简单的说，你需要大量的数据，很多的技巧才能训练好一个深层网络。这是个手艺活。

### 感知器定义

![](.\image\感知器.png)

感知器组成部分：

- **输入权值** ：一个感知器可以接收多个输入{x<sub>1</sub>,x<sub>2</sub>,...,x<sub>n</sub>|x<sub>i</sub>∈R}，每个输入上有一个**权值**w<sub>i</sub>∈R，此外还有一个**偏置项**b∈R，就是上图中的w<sub>0</sub>；
- **激活函数f(x)**：感知器的激活函数可以有很多选择，比如sigmoid函数；
- **输出**：感知器的输出由下面这个公式来计算。

$$
y=f(w*x+b)
$$

### 感知器训练

假设损失函数为均方差函数(MSE Mean Square Error)
$$
J(w,b)=\frac{1}{2m}\sum_{i=1}^{m}(a_{i}-y_{i})^{2}\\
J(w,b)=\frac{1}{2m}\sum_{i=1}^{m}(w_{0}x_{0}+w_{1}x_{1}+...+w_{n}x_{n}-y_{i})^{2}
$$
a<sub>i</sub>为预测值，y<sub>i</sub>为实际值。设m=1(训练样本为1条)时
$$
\frac{\partial J(w,b)}{\partial w_{1}}=x_{1}(w_{0}x_{0}+w_{1}x_{1}+...+w_{n}x_{n}-y_{i})
$$
其余参数导数求法相同。

使用梯度下降更新：
$$
w_{i}=w_{i}-\alpha \frac{\partial J(w_{i},b)}{\partial w_{i}}
$$
其中α为学习率。如果损失函数内为y<sub>i</sub>-a<sub>i</sub> ，则上面的梯度下降更新负号应为正号。

## 梯度下降优化算法

![](.\image\梯度下降示例图.png)

函数y=f(x)的极值点就是它的导数f<sup>‘</sup>(x)=0的那个点。因此我们可以通过解方程f<sup>‘</sup>(x)=0,求得函数的极值点。

对于计算机来说，随便选择一个点开始，比如上图的点x<sub>0</sub>。接下来，每次迭代修改的为x<sub>1</sub>,<sub>2</sub>,...，经过数次迭代后最终达到函数最小值点。

你可能要问了，为啥每次修改的值，都能往函数最小值那个方向前进呢？这里的奥秘在于，我们每次都是向函数y=f(x)的**梯度**的**相反方向**来修改。什么是**梯度**呢？翻开大学高数课的课本，我们会发现**梯度**是一个向量，它指向**函数值上升最快**的方向。显然，梯度的反方向当然就是函数值下降最快的方向了。我们每次沿着梯度相反方向去修改的值，当然就能走到函数的最小值附近。之所以是最小值附近而不是最小值那个点，是因为我们每次移动的步长不会那么恰到好处，有可能最后一次迭代走远了越过了最小值那个点。步长的选择是门手艺，如果选择小了，那么就会迭代很多轮才能走到最小值附近；如果选择大了，那可能就会越过最小值很远，收敛不到一个好的点上。

梯度下降算法的公式
$$
X_{new}=X_{old}-\alpha \bigtriangledown f(x)
$$
α为学习率（步长）

## 反向传播

### 神经元

神经元和感知器本质上是一样的，只不过我们说感知器的时候，它的激活函数是**阶跃函数**；而当我们说神经元时，激活函数往往选择为sigmoid函数或tanh函数。如下图所示：

![](.\image\sigmoid神经元.png)

计算一个神经元的输出的方法和计算一个感知器的输出是一样的。假设神经元的输入是向量$$\overrightarrow{x}​$$

 ，权重向量是$$\overrightarrow{w}$$(偏置项是w<sub>0</sub>)，激活函数是sigmoid函数，则其输出y：
$$
y=sigmoid(\vec{w}^{T}*\vec{x})\tag1
$$
sigmoid函数定义如下：
$$
sigmoid(x)=\frac{1}{1+e^{-x}}
$$
将其带入前面的式子，得到
$$
y=\frac{1}{1+e^{-\vec{w}^{T}\cdot \vec{x}}}
$$
sigmoid函数是非线性函数，值域是(0,1)。函数图像如下图所示

![](.\image\sigmoid.jpg)

sigmoid函数的导数是：
$$
令y=sigmoid(x)\\
y=\frac{1}{1+e^{-x}} \\
u(x)=1+e^{-x} \\
g(x)=e^{-x}\\
k(x)=-x\\
\frac{\mathrm{d} y}{\mathrm{d} x} = \frac{\mathrm{d} y}{\mathrm{d} u}\cdot \frac{\mathrm{d} u}{\mathrm{d} g}\cdot \frac{\mathrm{d} g}{\mathrm{d} k}\cdot \frac{\mathrm{d} k}{\mathrm{d} x}=-1\cdot u^{-2}\cdot 1\cdot e^{-x}\cdot -1=u^{-2}\cdot e^{-x}=\frac{e^{-x}}{(1+e^{-x})^{2}}=\frac{1}{1+e^{-x}}-\frac{1}{(1+e^{-x})^{2}}=y(1-y)\\
则y^{'}=y(1-y)
$$
可以看到，sigmoid函数的导数非常有趣，它可以用sigmoid函数自身来表示。这样，一旦计算出sigmoid函数的值，计算它的导数的值就非常方便。

### 神经网络

![](.\image\神经网络图2.png)

神经网络其实就是按照**一定规则**连接起来的多个**神经元**。上图展示了一个**全连接(full connected, FC)**神经网络，通过观察上面的图，我们可以发现它的规则包括：

- 神经元按照层来布局。最左边的层叫做**输入层**，负责接收输入数据；最右边的层叫**输出层**，我们可以从这层获取神经网络输出数据。输入层和输出层之间的层叫做**隐藏层**，因为它们对于外部来说是不可见的。
- 同一层的神经元之间没有连接。
- 第N层的每个神经元和第N-1层的**所有**神经元相连(这就是full connected的含义)，第N-1层神经元的输出就是第N层神经元的输入。
- 每个连接都有一个**权值**。

上面这些规则定义了全连接神经网络的结构。事实上还存在很多其它结构的神经网络，比如卷积神经网络(CNN)、循环神经网络(RNN)，他们都具有不同的连接规则。

### 计算神经网络输出

神经网络实际上就是一个输入向量$$\vec{x}$$到输出向量$$\vec{y}$$的函数，即：
$$
\vec{y}=f_{network}(\vec{x})
$$
根据输入计算神经网络的输出，需要首先将输入向量$$\vec{x}$$的每个元素$$x_{i}$$的值赋给神经网络的输入层的对应神经元，然后根据**式1**依次向前计算每一层的每个神经元的值，直到最后一层输出层的所有神经元的值计算完毕。最后，将输出层每个神经元的值串在一起就得到了输出向量$$\vec{y}$$。

接下来举一个例子来说明这个过程，我们先给神经网络的每个单元写上编号。

![](.\image\神经网络过程图.png)

如上图，输入层有三个节点，我们将其依次编号为1、2、3；隐藏层的4个节点，编号依次为4、5、6、7；最后输出层的两个节点编号为8、9。因为我们这个神经网络是**全连接**网络，所以可以看到每个节点都和**上一层的所有节点**有连接。比如，我们可以看到隐藏层的节点4，它和输入层的三个节点1、2、3之间都有连接，其连接上的权重分别为$$w_{41},w_{42},w_{43}​$$。那么，我们怎样计算节点4的输出值$$a_{4}​$$呢？

为了计算节点4的输出值，我们必须先得到其所有上游节点（也就是节点1、2、3）的输出值。节点1、2、3是**输入层**的节点，所以，他们的输出值就是输入向量本身。按照上图画出的对应关系，可以看到节点1、2、3的输出值分别是$$x_{1},x_{2},x_{3}$$。我们要求**输入向量的维度和输入层神经元个数相同**，而输入向量的某个元素对应到哪个输入节点是可以自由决定的，你偏非要把赋值给节点2也是完全没有问题的，但这样除了把自己弄晕之外，并没有什么价值。

一旦我们有了节点1、2、3的输出值，我们就可以根据**式1**计算节点4的输出值$$a_{4}$$：
$$
a_{4}=sigmoid(w_{41}x_{1}+w_{42}x_{2}+w_{43}x_{3}+w_{4b})
$$
上式的$$w_{4b}​$$是节点4的**偏置项**，图中没有画出来。而分别为节点1、2、3到节点4连接的权重，在给权重$$w_{ji}​$$编号时，我们把目标节点$$j​$$的编号放在前面，把源节点$$i​$$的编号放在后面。

同样，我们可以继续计算出节点5、6、7的输出值$$a_{5},a_{6},a_{7}$$。这样，隐藏层的4个节点的输出值就计算完成了，我们就可以接着计算输出层的节点8的输出值$$y_{1}$$：
$$
y_{1}=sigmoid(w_{84}a_{4}+w_{85}a_{5}+w_{86}a_{6}+w_{87}a_{7}+w_{8b})
$$
同理，我们还可以计算出$$y_{2}​$$的值。这样输出层所有节点的输出值计算完毕，我们就得到了在输入向量$$\vec{x}=\begin{bmatrix}
x_{1}\\ 
x_{2}\\ 
x_{3}
\end{bmatrix}​$$时，神经网络的输出向量$$\vec{y}=\begin{bmatrix}
y_{1}\\ 
y_{2}\\ 
\end{bmatrix}​$$。这里我们也看到，**输出向量的维度和输出层神经元个数相同**。

### 神经网络的矩阵表示

神经网络的计算如果用矩阵来表示会很方便（当然逼格也更高），我们先来看看隐藏层的矩阵表示。

首先我们把隐藏层4个节点的计算依次排列出来：
$$
a_{4}=sigmoid(w_{41}x_{1}+w_{42}x_{2}+w_{43}x_{3}+w_{4b})\\
a_{5}=sigmoid(w_{51}x_{1}+w_{52}x_{2}+w_{53}x_{3}+w_{5b})\\
a_{6}=sigmoid(w_{61}x_{1}+w_{62}x_{2}+w_{63}x_{3}+w_{6b})\\
a_{7}=sigmoid(w_{71}x_{1}+w_{72}x_{2}+w_{73}x_{3}+w_{7b})
$$
接着，定义网络的输入向量$$\vec{x}​$$和隐藏层每个节点的权重向量$$\vec{w_{j}}​$$。令
$$
\vec{x}=\begin{bmatrix}
x_{1}\\ 
x_{2}\\ 
x_{3}
\end{bmatrix}\\
\vec{w_{4}}=\begin{bmatrix}
w_{41}, &w_{42},  & w_{43}, & w_{4b}
\end{bmatrix}\\
\vec{w_{5}}=\begin{bmatrix}
w_{51}, &w_{52},  & w_{53}, & w_{5b}
\end{bmatrix}\\
\vec{w_{6}}=\begin{bmatrix}
w_{61}, &w_{62},  & w_{63}, & w_{6b}
\end{bmatrix}\\
\vec{w_{7}}=\begin{bmatrix}
w_{71}, &w_{72},  & w_{73}, & w_{7b}
\end{bmatrix}\\
f=sigmoid
$$
代入到前面的一组式子，得到：
$$
a_{4}=f(\vec{w_{4}}\cdot \vec{x})\\
a_{5}=f(\vec{w_{5}}\cdot \vec{x})\\
a_{6}=f(\vec{w_{6}}\cdot \vec{x})\\
a_{7}=f(\vec{w_{7}}\cdot \vec{x})\\
$$
现在，我们把上述计算$$a_{4},a_{5},a_{6},a_{7}$$的四个式子写到一个矩阵里面，每个式子作为矩阵的一行，就可以利用矩阵来表示它们的计算了。令
$$
\vec{a}=\begin{bmatrix}
a_{4}\\ 
a_{5}\\ 
a_{6}\\ 
a_{7}
\end{bmatrix}\\
W=\begin{bmatrix}
\vec{w_{4}}\\ 
\vec{w_{5}}\\ 
\vec{w_{6}}\\ 
\vec{w_{7}}
\end{bmatrix}=\begin{bmatrix}
w_{41} &w_{42}  & w_{43} &w_{4b} \\ 
w_{51} &w_{52}  & w_{53} &w_{5b} \\ 
w_{61} &w_{62}  & w_{63} &w_{6b} \\ 
w_{71} &w_{72}  & w_{73} &w_{7b} 
\end{bmatrix}\\
f(\begin{bmatrix}
x_{1}\\ 
x_{2}\\ 
x_{3}\\ 
.\\ 
.\\ 
.
\end{bmatrix})=\begin{bmatrix}
f(x_{1})\\ 
f(x_{2})\\ 
f(x_{3})\\ 
.\\ 
.\\ 
.
\end{bmatrix}
$$
带入前面的一组式子，得到
$$
\vec{a}=f(W\cdot\vec{x}) \tag2
$$
在**式2**中，是激活函数，在本例中是sigmoid函数；W是某一层的权重矩阵；$$\vec{x}$$是某层的输入向量；$$\vec{a}$$是某层的输出向量。**式2**说明神经网络的每一层的作用实际上就是先将输入向量**左乘**一个数组进行线性变换，得到一个新的向量，然后再对这个向量**逐元素**应用一个激活函数。

每一层的算法都是一样的。比如，对于包含一个输入层，一个输出层和三个隐藏层的神经网络，我们假设其权重矩阵分别为$$W_{1},W_{2},W_{3},W_{4}$$，每个隐藏层的输出分别是$$\vec{a_{1}},\vec{a_{2}},\vec{a_{3}}$$，神经网络的输入为$$\vec{x}$$，神经网络的输出为$$\vec{y}$$，如下图所示：

![](.\image\深层神经网络.png)

则每一层的输出向量的计算可以表示为：
$$
\vec{a_{1}}=f(W_{1}\cdot \vec{x})\\
\vec{a_{2}}=f(W_{2}\cdot \vec{a_{1}})\\
\vec{a_{3}}=f(W_{3}\cdot \vec{a_{2}})\\
\vec{y}=f(W_{4}\cdot \vec{a_{3}})
$$
这就是神经网络输出值的计算方法。

### 神经网络的训练

现在，我们需要知道一个神经网络的每个连接上的权值是如何得到的。我们可以说神经网络是一个**模型**，那么这些权值就是模型的**参数**，也就是模型要学习的东西。然而，一个神经网络的连接方式、网络的层数、每层的节点数这些参数，则不是学习出来的，而是人为事先设置的。对于这些人为设置的参数，我们称之为**超参数(Hyper-Parameters)**。

接下来，我们将要介绍神经网络的训练算法：反向传播算法。

### 反向传播算法

我们首先直观的介绍反向传播算法，最后再来介绍这个算法的推导。当然读者也可以完全跳过推导部分，因为即使不知道如何推导，也不影响你写出来一个神经网络的训练代码。事实上，现在神经网络成熟的开源实现多如牛毛，除了练手之外，你可能都没有机会需要去写一个神经网络。

我们设神经元的激活函数f为函数sigmoid函数。

我们假设每个训练样本为$$(\vec{x},\vec{t})​$$，其中向量$$\vec{x}​$$是训练样本的特征，而$$\vec{t}​$$是样本的目标值。

![](.\image\反向传播.png)

首先，我们根据上一节介绍的算法，用样本的特征$$\vec{x}$$，计算出神经网络中每个隐藏层节点的输出$$a_{i}$$，以及输出层每个节点的输出$$y_{i}​$$。

我们取网络所有输出层节点的误差平方和作为目标函数：
$$
E_{d}=\frac{1}{2}\sum_{i=1}^{m}(t_{i}-y_{i})^{2}
$$
其中$$m$$表示输出节点数目，$$E_{d}$$表示样本d的误差。

然后，我们用**随机梯度下降**算法对目标函数进行优化：
$$
w_{ji}\leftarrow w_{ji}-\eta\frac{\partial E_{d}}{\partial w_{ji}}
$$
随机梯度下降算法也就是需要求出误差对于每个权重的偏导数（也就是梯度），怎么求呢？

![](.\image\神经网络过程图.png)

观察上图，我们发现权重$$w_{ji}$$仅能通过影响节点$$j$$的输入值影响网络的其它部分，设$$net_{j}$$是节点$$j​$$的**加权输入**，即
$$
net_{j}=\vec{w_{j}}\cdot \vec{x_{j}}=\sum_{i=1}^{n}w_{ji}x_{ji}
$$
$$E_{d}$$是$$net_{j}$$的函数，而$$net_{j}$$是$$w_{ji}$$的函数。根据链式求导法则，可以得到：
$$
\frac{\partial E_{d}}{\partial w_{ji}}=\frac{\partial E_{d}}{\partial net_{j}}\frac{\partial net_{j}}{\partial w_{ji}}=\frac{\partial E_{d}}{\partial net_{j}}\frac{\partial \sum _{i}w_{ji}x_{ji}}{\partial w_{ji}}=\frac{\partial E_{d}}{\partial net_{j}}x_{ji}
$$
上式中，$$x_{ji}$$是节点$$i$$传递给节点$$j$$的输入值，也就是节点$$i$$的输出值。

对于$$\frac{\partial E_{d}}{\partial net_{j}}​$$的推导，需要区分**输出层**和**隐藏层**两种情况。

#### 输出层权值训练

对于**输出层**来说，$$net_{j}$$仅能通过节点$$j$$的输出值$$y_{j}$$来影响网络其它部分，也就是说$$E_{d}$$是$$y_{j}$$的函数，而$$y_{j}$$是$$net_{j}$$的函数，其中$$y_{j}=sigmoid(net_{j})​$$。所以我们可以再次使用链式求导法则：
$$
\frac{\partial E_{d}}{\partial net_{j}}=\frac{\partial E_{d}}{\partial y_{j}}\frac{\partial y_{j}}{\partial net_{j}}
$$
考虑上式第一项:
$$
\frac{\partial E_{d}}{\partial y_{j}}=\frac{\partial \frac{1}{2}\sum (t_{j}-y_{j})^{2}}{\partial y_{j}}=-(t_{j}-y_{j})
$$
考虑上式第二项：
$$
\frac{\partial y_{j}}{\partial net_{j}}=\frac{\partial sigmoid(net_{j})}{\partial net_{j}}=y_{j}(1-y_{j})
$$
将第一项和第二项带入，得到：
$$
\frac{\partial E_{d}}{\partial net_{j}}=-(t_{j}-y_{j})y_{j}(1-y_{j})
$$
如果令$$\delta _{j}=-\frac{\partial E_{d}}{\partial net_{j}}​$$，也就是一个节点的误差项是网络误差$$\delta​$$对这个节点输入的偏导数的相反数。带入上式，得到：
$$
\delta _{j}=(t_{j}-y_{j})y_{j}(1-y_{j})
$$
将上述推导带入随机梯度下降公式，得到：
$$
w_{ji}\leftarrow w_{ji}-\eta \frac{\partial E_{d}}{\partial w_{ji}}=w_{ji}-\eta \frac{\partial E_{d}}{\partial net_{ji}}\frac{\partial net_{ji}}{\partial w_{ji}}=w_{ji}+\eta (t_{j}-y_{j})y_{j}(1-y_{j})(1-y_{j})x_{ji}=w_{ji}+\eta \delta _{j}x_{ji}
$$

#### 隐藏层权值训练

现在我们要推导出隐藏层的$$\frac{\partial E_{d}}{\partial net_{j}}​$$。

首先，我们需要定义节点$$j​$$的所有直接下游节点的集合$$Downstream(j)​$$。例如，对于节点4来说，它的直接下游节点是节点8、节点9。可以看到$$net_{j}​$$只能通过影响$$Downstream(j)​$$再影响$$E_{d}​$$。设$$net_{k}​$$是节点$$j​$$的下游节点的输入，则$$E_{d}​$$是$$net_{k}​$$的函数，而$$net_{k}​$$是$$net_{j}​$$的函数。因为$$net_{k}​$$有多个，我们应用全导数公式，可以做出如下推导：
$$
\frac{\partial E_{d}}{\partial net_{j}}\\
=\sum_{k\in Downstream(j)}\frac{\partial E_{d}}{\partial net_{k}}\frac{\partial net_{k}}{\partial net_{j}}\\
=\sum_{k\in Downstream(j)}-\delta _{k}\frac{\partial net_{k}}{\partial net_{j}}\\
=\sum_{k\in Downstream(j)}-\delta _{k}\frac{\partial net_{k}}{\partial a_{j}}\frac{\partial a_{j}}{\partial net_{j}}\\
=\sum_{k\in Downstream(j)}-\delta _{k}w_{kj}\frac{\partial a_{j}}{\partial net_{j}}\\
=\sum_{k\in Downstream(j)}-\delta _{k}w_{kj}a_{j}(1-a_{j})\\
=-a_{j}(1-a_{j})\sum_{k\in Downstream(j)}\delta _{k}w_{kj}
$$
因为$$\delta _{j}=-\frac{\partial E_{d}}{\partial net_{j}}$$带入上式得到：
$$
\delta _{j}=a_{j}(1-a_{j})\sum_{k\in Downstream(j)}\delta _{k}w_{kj}
$$
其中$$a_{j}$$为激活函数。

## 卷积神经网络

### 全连接网络VS卷积网络

全连接神经网络之所以不太适合图像识别任务，主要有以下几个方面的问题：

- **参数数量太多** 考虑一个输入$$1000*1000$$像素的图片(一百万像素，现在已经不能算大图了)，输入层有$$1000*1000=100$$万节点。假设第一个隐藏层有100个节点(这个数量并不多)，那么仅这一层就有$$(1000*1000+1)*100=1​$$亿​参数，这实在是太多了！我们看到图像只扩大一点，参数数量就会多很多，因此它的扩展性很差。
- **没有利用像素之间的位置信息** 对于图像识别任务来说，每个像素和其周围像素的联系是比较紧密的，和离得很远的像素的联系可能就很小了。如果一个神经元和上一层所有神经元相连，那么就相当于对于一个像素来说，把图像的所有像素都等同看待，这不符合前面的假设。当我们完成每个连接权重的学习之后，最终可能会发现，有大量的权重，它们的值都是很小的(也就是这些连接其实无关紧要)。努力学习大量并不重要的权重，这样的学习必将是非常低效的。
- **网络层数限制** 我们知道网络层数越多其表达能力越强，但是通过梯度下降方法训练深度全连接神经网络很困难，因为全连接神经网络的梯度很难传递超过3层。因此，我们不可能得到一个很深的全连接神经网络，也就限制了它的能力。

那么，卷积神经网络又是怎样解决这个问题的呢？主要有三个思路：

- **局部连接** 这个是最容易想到的，每个神经元不再和上一层的所有神经元相连，而只和一小部分神经元相连。这样就减少了很多参数。
- **权值共享** 一组连接可以共享同一个权重，而不是每个连接有一个不同的权重，这样又减少了很多参数。
- **下采样** 可以使用Pooling来减少每层的样本数，进一步减少参数数量，同时还可以提升模型的鲁棒性。

对于图像识别任务来说，卷积神经网络通过尽可能保留重要的参数，去掉大量不重要的参数，来达到更好的学习效果。

### 卷积神经网络结构

![](.\image\卷积神经网络案例图.png)

三维的层结构

从**图中**我们可以发现**卷积神经网络**的层结构和**全连接神经网络**的层结构有很大不同。**全连接神经网络**每层的神经元是按照**一维**排列的，也就是排成一条线的样子；而**卷积神经网络**每层的神经元是按照**三维**排列的，也就是排成一个长方体的样子，有**宽度**、**高度**和**深度**。

对于**图中**展示的神经网络，我们看到输入层的宽度和高度对应于输入图像的宽度和高度，而它的深度为1。接着，第一个卷积层对这幅图像进行了卷积操作(后面我们会讲如何计算卷积)，得到了三个Feature Map。这里的"3"可能是让很多初学者迷惑的地方，实际上，就是这个卷积层包含三个Filter，也就是三套参数，每个Filter都可以把原始输入图像卷积得到一个Feature Map，三个Filter就可以得到三个Feature Map。至于一个卷积层可以有多少个Filter，那是可以自由设定的。也就是说，卷积层的Filter个数也是一个**超参数**。我们可以把Feature Map可以看做是通过卷积变换提取到的图像特征，三个Filter就对原始图像提取出三组不同的特征，也就是得到了三个Feature Map，也称做三个**通道(channel)**。

继续观察**图**，在第一个卷积层之后，Pooling层对三个Feature Map做了**下采样**(后面我们会讲如何计算下采样)，得到了三个更小的Feature Map。接着，是第二个**卷积层**，它有5个Filter。每个Fitler都把前面**下采样**之后的**3个\**Feature Map**卷积**在一起，得到一个新的Feature Map。这样，5个Filter就得到了5个Feature Map。接着，是第二个Pooling，继续对5个Feature Map进行**下采样**，得到了5个更小的Feature Map。

如图所示网络的最后两层是全连接层。第一个全连接层的每个神经元，和上一层5个Feature Map中的每个神经元相连，第二个全连接层(也就是输出层)的每个神经元，则和第一个全连接层的每个神经元相连，这样得到了整个网络的输出。

至此，我们对**卷积神经网络**有了最基本的感性认识。接下来，我们将介绍**卷积神经网络**中各种层的计算和训练。

### 卷积神经网络输出值的计算

#### 卷积层输出值的计算

我们用一个简单的例子来讲述如何计算**卷积**，然后，我们抽象出**卷积层**的一些重要概念和计算方法。

假设有一个5*5的图像，使用一个3*3的filter进行卷积，想得到一个3*3的Feature Map，如下所示：

![](.\image\卷积神经网络案例.png)

为了清楚的描述**卷积**计算过程，我们首先对图像的每个像素进行编号，用$$x_{i,j}$$表示图像的第行第列元素；对filter的每个权重进行编号，用$$w_{m,n}$$表示第$$m$$行第$$n$$列权重，用$$w_{b}$$表示filter的**偏置项**；对Feature Map的每个元素进行编号，用$$a_{i,j}$$表示Feature Map的第$$i$$行第$$j$$列元素；用$$f$$表示**激活函数**(这个例子选择**relu函数**作为激活函数)。然后，使用下列公式计算卷积：
$$
a_{i,j}=f(\sum_{m=0}^{2}\sum_{n=0}^{2}w_{m,n}x_{i+m,j+n}+w_{b})
$$
例如，对于Feature Map左上角元素来说，其卷积计算方法为：
$$
a_{0,0}=f(\sum_{m=0}^{2}\sum_{n=0}^{2}w_{m,n}x_{m+0,n+0}+w_{b})\\
=relu(w_{0,0}x_{0,0}+w_{0,1}x_{0,1}+w_{0,2}x_{0,2}+w_{1,0}x_{1,0}+w_{1,1}x_{1,1}+w_{1,2}x_{1,2}+w_{2,0}x_{2,0}+w_{2,1}x_{2,1}+w_{2})\\
=relu(1+0+1+0+1+0+0+0+1+0)=relu(4)=4
$$
计算结果如下图所示：

![](.\image\卷积过程.png)

###  权值共享

神经元的偏置部分也是同一种滤波器共享的。 比如卷积核是三层，每一层使用的偏置项都是相等的。

## 对抗生成网络(GAN)



## 优化器

### Batch Gradient Descent(BGD，批量梯度下降)

BGD训练过程中每次迭代使用所有样本来进行梯度的更新。

#### 优点

- 一次迭代是对所有样本进行计算，此时利用矩阵进行操作，实现了并行；
- 由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，BGD一定能够得到全局最优。

#### 缺点

- 当样本数目很大时，每迭代一步都需要对所有样本计算，训练过程会很慢。

### Stochastic Gradient Descent(SGD,随机梯度下降)

SGD训练过程中每次迭代使用一个样本来对参数进行更新。

#### 优点

- 由于不是在全部训练数据上的损失函数，而是在每轮迭代中，随机优化某一条训练数据上的损失函数，这样每一轮参数的更新速度大大加快。

#### 缺点

- 准确度下降。由于即使在目标函数为强凸函数的情况下，SGD仍旧无法做到线性收敛；
- 可能会收敛到局部最优，由于单个样本并不能代表全体样本的趋势；
- 不易于并行实现。

### Mini-Batch Gradient Descent(MBGD,小批量梯度下降)

MBGD训练过程中每次迭代使用**batch size**个样本来对参数进行更新。它是对BGD以及SGD的一个折中办法。

#### 优点

- 通过矩阵运算，每次在一个batch上优化神经网络参数并不会比单个数据慢太多；
- 每次使用一个batch可以大大减小收敛所需要的迭代次数，同时可以使收敛到的结果更加接近梯度下降的效果；
- 可实现并行化。

#### 缺点

- batch size的不当选择可能会带来一些问题。

  batcha size的选择带来的影响：

  - 在合理的范围内，增大batch_size的好处：

    - 内存利用率提高了，大矩阵乘法的并行化效率提高。

    - 跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。

    - 在一定范围内，一般来说 batch size 越大，其确定的下降方向越准，引起训练震荡越小。

  - 盲目增大batch size的坏处

    - 内存利用率提高了，但是内存容量可能撑不住了。
    - 跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。
    - batch size 增大到一定程度，其确定的下降方向已经基本不再变化

    

### Momentum

SGD参数更新公式为：  
$$
W:=W-\alpha d_{w}\\
b:=b-\alpha d_{b}
$$
它的梯度更新路线上下波动很大，收敛速度很慢。因此根据这些原因，有人提出了Momentum优化算法，这个是基于SGD的，简单理解就是为了防止波动，它主要是基于梯度的移动指数加权平均来更新参数。引进超参数beta(一般取0.9)。

参数更新公式为：  
$$
V_{dw}=\beta V_{dw}+(1-\beta )dW\\
V_{db}=\beta V_{db}+(1-\beta )db\\
W:=W-\alpha {V_{dw}}\\
b:=b-\alpha {V_{db}}
$$

### Nesterov Momentum

Nesterov Momentum是对Momentum的改进，可以理解为nesterov动量在标准动量方法中添加了一个**校正因子**。

### Root Mean Square Prop(RMSProp)

RMSProp思想与Momentum相似，也用到权重超参数beta（一般取0.999）。

参数更新公式为：
$$
S_{dw}=\beta S_{dw}+(1-\beta )dW^{2}\\
S_{db}=\beta S_{db}+(1-\beta )db^{2}\\
W:=W-\alpha \frac{dW}{\sqrt{S_{dw}}}\\
b:=b-\alpha \frac{db}{\sqrt{S_{db}}}
$$
为了防止分母为0，在分数下加上个特别小的值epsilon，通常选取10^-8。

### Adagrad

大多数优化器训练参数更新过程中都使用了相同的学习率α。Adagrad能够在训练中自动的对learning rate进行调整，对于出现频率较低的参数采用较大的α更新，相反，对于出现频率较高的参数采用较小的α更新。因此，**Adagrad非常适合处理稀疏数据**。

如果是普通的SGD，那么每一时刻梯度的更新公式为：
$$
\Theta _{t+1}=\Theta _{t,i}-\alpha *g_{t,i}
$$
g<sub>t,i</sub>为第t轮第i个参数的梯度。θ<sub>t,i</sub> 为参数值

Adagrad在每轮训练中对每个参数θ<sub>i</sub> 进行更新，参数更新公式为：
$$
\Theta _{t+1,i}=\Theta _{t,i}-\frac{\alpha }{\sqrt{G_{t,ii}+\varepsilon }}*g_{t,i}
$$
G<sub>t</sub>为对角矩阵，大小为D*D。每个对角线位置i,i为对应参数θ<sub>i</sub> 从第一轮到第t轮梯度的平方和。varepsilon 是平滑项，用于避免分母为0，一般取值为10^-8。Adagrad的缺点是在训练的中后期，分母上梯度平方的累加会越来越大，从而梯度趋近于0，使得训练提前结束。

### Adadelta

Adadelta 是 Adagrad 的一个具有更强鲁棒性的的扩展版本，它不是累积所有过去的梯度，而是根据渐变更新的移动窗口调整学习速率。 这样，即使进行了许多更新，Adadelta 仍在继续学习。   

与Adagrad 相比，就是分母的 G 换成了过去的梯度平方的衰减平均值，**指数衰减平均值**。

这个分母相当于**梯度的均方根 root mean squared (RMS)**，在数据统计分析中，将所有值平方求和，求其均值，再开平方，就得到均方根值 。

#### 优点

- 防止学习率衰减或梯度消失等问题的出现。

### Adam

该优化器相当于RMSprop+Momentum。

参数更新公式为：
$$
V_{dw}=\beta _{1} V_{dw}+(1-\beta _{1} )dW\\
V_{db}=\beta _{1} V_{db}+(1-\beta _{1} )db\\
S_{dw}=\beta_{2} S_{dw}+(1-\beta_{2} )dW^{2}\\
S_{db}=\beta_{2} S_{db}+(1-\beta_{2} )db^{2}\\
V_{dW}^{corrected}=\frac{V_{dW}}{1-\beta _{1}^{t}}\\
V_{db}^{corrected}=\frac{V_{db}}{1-\beta _{1}^{t}}\\
S_{dW}^{corrected}=\frac{S_{dW}}{1-\beta _{2}^{t}}\\
S_{db}^{corrected}=\frac{S_{db}}{1-\beta _{2}^{t}}\\
W:=W-\alpha \frac{V_{dW}}{\sqrt{S_{dW}^{corrected}}}\\
b:=b-\alpha \frac{V_{db}}{\sqrt{S_{db}^{corrected}}}
$$
beta1一般为0.9，beta2一般为0.9999。在实际应用中，Adam方法效果良好。与其他自适应学习率算法相比，其收敛速度更快，学习效果更为有效，而且可以纠正其他优化技术中存在的问题，如学习率消失、收敛过慢或是高方差的参数更新导致损失函数波动较大等问题。

## 激活函数

[激活函数详解](<https://zhuanlan.zhihu.com/p/22142013>)

激活函数一般用于神经网络的层与层之间，将上一层的输出转换之后输入到下一层。如果没有激活函数引入的额非线性特性，那么神经网络就只相当于原始感知机的矩阵相乘。  

### Sigmoid

![](.\image\sigmoid.jpg)

sigmoid在定义域内处处可导，且两侧导数逐渐趋近于0。

#### 缺点

- 激活函数计算量大，反向传播求误差梯度时，求导涉及除法；
- 反向传播时，很容易就会出现梯度消失的情况，从而无法完成深层网络的训练；
- Sigmoids函数饱和且kill掉梯度；
- Sigmoids函数收敛缓慢。

### tanh

![](.\image\tanh.jpg)

### Relu

![](.\image\relu.jpg)

#### 优点

- **速度快** 和sigmoid函数需要计算指数和倒数相比，relu函数其实就是一个max(0,x)，计算代价小很多。
- **减轻梯度消失问题** relu函数的导数是1，不会导致梯度变小。当然，激活函数仅仅是导致梯度减小的一个因素，但无论如何在这方面relu的表现强于sigmoid。使用relu激活函数可以让你训练更深的网络。
- **稀疏性** 通过对大脑的研究发现，大脑在工作的时候只有大约5%的神经元是激活的，而采用sigmoid激活函数的人工神经网络，其激活率大约是50%。有论文声称人工神经网络在15%-30%的激活率时是比较理想的。因为relu函数在输入小于0时是完全不激活的，因此可以获得一个更低的激活率。

### 缺点

- 训练的时候很”脆弱”，很容易就”die”了。

### PRelu

![](.\image\PRelu.jpg)

### RReLU

### Maxout

### ELU

![](.\image\elu.jpg)



## 问题及解决方法

### 梯度爆炸与梯度消失

深度神经网络训练过程中，使用了反向传播的方式更新参数，该方式基于的是链式求导。计算每层梯度的时候回设计一些连乘操作，如果网络过深，当连乘的因子大部分小于1时，最后的乘积结果可能趋于0，会导致后面的网络层参数不发生变化，不能继续进行学习（梯度消失）。当连乘的因子大部分大于1，最后的乘积结果可能趋于无穷，会导致后面的网络层参数变化过大，导致Loss值出现震荡，收敛不到最低值的情况（梯度爆炸）。

#### 梯度爆炸解决办法

- 降低学习率

- 梯度裁剪（Gradient Clipping）

  如果梯度特别大，那么将其投影到一个比较小的尺度上。

### 线性与非线性

在数学上可理解为一阶导数为常数的函数为线性函数，一阶导数不为常数的函数为非线性函数。

### 为什么RNN一般情况下为等长的

为了让多条数据合并成矩阵进行运算，能够使用并行处理。如果不等长则不能合并为矩阵。tensorflow支持同一批训练数据等长的训练接口。

### Padding 等于SAME和VALID

Padding运算作用于输入向量的每一维，每一维的操作都是一致的，所以理解Padding的操作，只需要理解一维向量的padding过程

假设一个一维向量，输入形状为input_size，经过滤波操作后的输出形状为output_size，滤波窗口为filter_size，需要padding的个数为padding_needed，滤波窗口滑动步长为stride，则之间满足关系：
$$
output_size=(input_size+padding_needed-filter_size)/stride+1
$$
由公式可知，指定padding_needed可以确定output_size的值，反过来，如果已知输出的形状，则进而可以确定padding的数量。

这是两种处理padding的方案，pytorch采用的是第一种，即在卷积或池化时先确定padding数量，自动推导输出形状；tensorflow和caffe采用的是更为人熟知的第二种，即先根据Valid还是Same确定输出大小，再自动确定padding的数量

Valid和Same是预设的两种padding模式，Valid指不padding，same指输出大小尽可能和输入大小成比例

下面是tensorflow计算padding的代码：

```
作者：JamesPlur
链接：https://zhuanlan.zhihu.com/p/73118626
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

void GetWindowedOutputSize(int64_t input_size, int32_t filter_size, int32_t dilation_rate,
                           int32_t stride, const std::string& padding_type, 
                           int64_t* output_size,int32_t* padding_before, 
                           int32_t* padding_after) {
  CHECK_GT(stride, 0);
  CHECK_GE(dilation_rate, 1);

  int32_t effective_filter_size = (filter_size - 1) * dilation_rate + 1;
  if (padding_type == "valid") {
    if (output_size) { *output_size = (input_size - effective_filter_size + stride) / stride; }
    if (padding_before) { *padding_before = 0; }
    if (padding_after) { *padding_after = 0; }
  } else if (padding_type == "same") {
    int64_t tmp_output_size = (input_size + stride - 1) / stride;
    if (output_size) { *output_size = tmp_output_size; }
    const int32_t padding_needed = std::max(
        0,
        static_cast<int32_t>((tmp_output_size - 1) * stride + effective_filter_size - input_size));
    // For odd values of total padding, add more padding at the 'right'
    // side of the given dimension.
    if (padding_before) { *padding_before = padding_needed / 2; }
    if (padding_after) { *padding_after = padding_needed - padding_needed / 2; }
  } else {
    UNIMPLEMENTED();
  }
  if (output_size) { CHECK_GE((*output_size), 0); }
}
```

[参考]: https://www.zhihu.com/search?type=content&amp;q=padding%3DSAME



## 论文阅读

## AlexNet深度卷积神经网络

### 深度卷积神经网络图像集分类

#### 摘要

我们训练了一个庞大的深层卷积神经网络，将ImageNet LSVRC-2010比赛中的120万张高分辨率图像分为1000个不同的类别。在测试数据上，我们取得了37.5％和17.0％的前1和前5的错误率，这比以前的先进水平要好得多。具有6000万个参数和650,000个神经元的神经网络由五个卷积层组成，其中一些随后是最大池化层，三个全连接层以及最后的1000个softmax输出。为了加快训练速度，我们使用非饱和神经元和能高效进行卷积运算的GPU实现。为了减少全连接层中的过拟合，我们采用了最近开发的称为“dropout”的正则化方法，该方法证明是非常有效的。我们还在ILSVRC-2012比赛中使用了这种模式的一个变种，取得了15.3％的前五名测试失误率，而第二名的成绩是26.2％。

#### 1 介绍

目前，机器学习方法对物体识别非常重要。为了改善他们的表现，我们可以收集更大的数据集，训练更强大的模型，并使用更好的技术来防止过拟合。直到最近，标记好图像的数据集相对还较小——大约上万的数量级（例如，NORB [16]，Caltech-101/256 [8,9]和CIFAR-10/100 [12]）。使用这种规模的数据集可以很好地解决简单的识别任务，特别是如果他们增加了保留标签转换（label-preserving transformations）。例如，目前MNIST数字识别任务的最低错误率（<0.3％）基本达到了人类的识别水平[4]。但是物体在现实环境中可能表现出相当大的变化性，所以要学会识别它们，就必须使用更大的训练集。事实上，小图像数据集的缺点已是众所周知（例如，Pinto[21]），但直到最近才可以收集到数百万的标记数据集。新的大型数据集包括LabelMe [23]，其中包含数十万个完全分割的图像，以及ImageNet [6]，其中包含超过15,000万个超过22,000个类别的高分辨率图像。
要从数百万图像中学习数千个类别，我们需要一个具有强大学习能力的模型。然而，物体识别任务的巨大复杂性意味着即使是像ImageNet这样大的数据集也不能完美地解决这个问题，所以我们的模型也需要使用很多先验知识来弥补我们数据集不足的问题。卷积神经网络（CNN）就构成了一类这样的模型[16,11,13,18,15,22,26]。它们的容量可以通过改变它们的深度和宽度来控制，并且它们也对图像的性质（即统计量的定态假设以及像素局部依赖性假设）做出准确而且全面的假设。因此，与具有相同大小的层的标准前馈神经网络相比，CNN具有更少的连接和参数，因此它们更容易训练，而其理论最优性能可能稍微弱一些。
尽管CNN具有很好的质量，并且尽管其局部结构的效率相对较高，但将它们大规模应用于高分辨率图像时仍然显得非常昂贵。幸运的是，当前的GPU可以用于高度优化的二维卷积，能够加速许多大型CNN的训练，并且最近的数据集（如ImageNet）包含足够多的标记样本来训练此类模型，而不会出现严重的过度拟合。
本文的具体贡献如下：我们在ILSVRC-2010和ILSVRC-2012比赛中使用的ImageNet子集上训练了迄今为止最大的卷积神经网络之一[2]，并在这些数据集上取得了迄今为止最好的结果。我们编写了一个高度优化的2D卷积的GPU实现以及其他训练卷积神经网络的固有操作，并将其公开。我们的网络包含许多新的和不同寻常的功能，这些功能可以提高网络的性能并缩短训练时间，详情请参阅第3节。我们的网络规模较大，即使有120万个带标签的训练样本，仍然存在过拟合的问题，所以我们采用了几个有效的技巧来阻止过拟合，在第4节中有详细的描述。我们最终的网络包含五个卷积层和三个全连接层，并且这个深度似乎很重要：我们发现去除任何卷积层（每个卷积层只包含不超过整个模型参数的1%的参数）都会使网络的性能变差。
最后，网络的规模主要受限于目前GPU上可用的内存量以及我们可接受的训练时间。我们的网络需要在两块GTX 580 3GB GPU上花费五到六天的时间来训练。我们所有的实验都表明，通过等待更快的GPU和更大的数据集出现，我们的结果可以进一步完善。

#### 2 数据集

ImageNet是一个拥有超过1500万个已标记高分辨率图像的数据集，大概有22,000个类别。图像都是从网上收集，并使用Amazon-Mechanical Turk群智工具人工标记。从2010年起，作为Pascal视觉对象挑战赛的一部分，这是每年举办一次的名为ImageNet大型视觉识别挑战赛（ILSVRC）的比赛。 ILSVRC使用的是ImageNet的一个子集，每1000个类别中大约有1000个图像。总共有大约120万张训练图像，50,000张验证图像和150,000张测试图像。
ILSVRC-2010是ILSVRC中的唯一可以使用测试集标签的版本，因此这也正是我们进行大部分实验的版本。由于我们也在ILSVRC-2012比赛中引入了我们的模型，因此在第6部分中，我们也会给出此版本数据集的结果，尽管这个版本的测试集标签不可用。在ImageNet上，习惯上使用两种错误率：top-1和top-5，其中top-5错误率是正确标签不在被模型认为最可能的五个标签之中的测试图像的百分率。
ImageNet由可变分辨率的图像组成，而我们的系统需要固定的输入尺寸。因此，我们将图像下采样到256×256的固定分辨率。给定一个矩形图像，我们首先重新缩放图像，使得短边长度为256，然后从结果中裁剪出中心的256×256的图片。除了将每个像素中减去训练集的像素均值之外，我们没有以任何其他方式对图像进行预处理。所以我们在像素的（中心）原始RGB值上训练了我们的网络。

#### 3 结构

图2概括了我们所提出网络的结构。它包含八个学习层——五个卷积层和三个全连接层。下面，我们将描述一些所提出网络框架中新颖或不寻常的地方。 3.1-3.4节按照我们对它们重要性的估计进行排序，其中最重要的是第一个。

##### 3.1 ReLU非线性单元

对一个神经元模型的输出的常规套路是，给他接上一个激活函数：$$f(x)=tanh(x)$$或者$$f(x)=(1+e^{-x})^{-1}$$。就梯度下降法的训练时间而言，这些饱和非线性函数比非饱和非线性函数如$$f(x)=max(0,x)$$慢得多。根据Nair和Hinton的说法[20]，我们将这种非线性单元称为——修正非线性单元（Rectified Linear Units (ReLUs)）。使用ReLUs做为激活函数的卷积神经网络比起使用tanh单元作为激活函数的训练起来快了好几倍。这个结果从图1中可以看出来，该图展示了对于一个特定的四层CNN，CIFAR-10数据集训练中的误差率达到25%所需要的迭代次数。从这张图的结果可以看出，如果我们使用传统的饱和神经元模型来训练CNN，那么我们将无法为这项工作训练如此大型的神经网络。

**饱和激活函数会压缩输入值**：如$$f(x)=max(0,x)$$，当$$x$$趋于正无穷则$$f(x)$$也趋于正无穷，所以该函数是非饱和的，$$sigmoid$$函数的范围是$$[0,1]$$所以是饱和的，$$tanh$$函数也是饱和的，因为其取值范围为$$[-1,1]$$。

## 文本分类

[文本分类模型实现](<https://github.com/DX2048/text_classification>)

### FastText

[参考](<https://zhuanlan.zhihu.com/p/32965521>)

### TextCNN



### TextRNN

### RCNN

### Hierarchical Attention Network

### Seq2seq With Attention

### Dynamic Memory Network

### EntityNetwork:tracking state of the world

### Ensemble models

### Transformer("Attend Is All You Need")

## 如何选择优化算法

如果数据是稀疏的，就用自适用方法，即 Adagrad, Adadelta, RMSprop, Adam。

RMSprop, Adadelta, Adam 在很多情况下的效果是相似的。

Adam 就是在 RMSprop 的基础上加了 bias-correction 和 momentum。

随着梯度变的稀疏，Adam 比 RMSprop 效果会好。

整体来讲，**Adam 是最好的选择**。



## 引用

[零基础入门深度学习](<https://www.zybuluo.com/hanbingtao/note/433855>)