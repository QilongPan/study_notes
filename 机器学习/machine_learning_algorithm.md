# 机器学习

机器学习学科致力于研究如何通过计算的手段，利用经验来改善系统自身的性能。在计算机系统中，“经验”通常以“数据”形式存在。

通常假设样本空间中全体样本服从一个未知的“分布”，我们获得的每个样本都是独立地从这个分布式采样获得的，即“独立同分布”。假如模型的泛化能力很差，可以查看训练集与测试集的分布是否一致，可以采用如下方式：

import seaborn as sns 
import matplotlib.pyplot as plt
%matplotlib inline
for column in df_pcap_features.columns:
    if column == 'target':
        continue
    plt.figure(figsize=(10,5))
    sns.kdeplot(df_pcap_features[column])
    sns.kdeplot(df_pcap_features_test[column])

**归纳**：从特殊到一般的“泛化”过程，即从具体的事实归结出一般性规律。

**演绎**：从一般到特殊的“特化”过程，即从基础原理推演出具体状况。

## 监督学习  

### 分类

#### 支持向量机(SVM)

##### 拉格朗日乘子法

- 无约束条件

  解决办法通常是对函数变量求导，得到令导函数为0的点（极值点），将结果带回原函数进行验证即可。

- 等式约束条件
  $$
  \begin{aligned} 
  min\ \ &f(x)\\
  s.t.\ &h_{k}(x)=0\ \ \ \ k=1,2,...,l
  \end{aligned}
  $$
  s.t.表示subject to,"受限于"的意思。$$l$$表示有$$l$$个约束条件。

  解决办法是消元法或者拉格朗日法。

  例如给定椭球：
  $$
  \frac{x^{2}}{a^{2}}+\frac{y^{2}}{b^{2}}+\frac{z^{2}}{c^{2}}=1
  $$
  求这个椭球的内接长方体的最大体积。这个问题实际上就是条件极值问题，即在条件$$\frac{x^{2}}{a^{2}}+\frac{y^{2}}{b^{2}}+\frac{z^{2}}{c^{2}}=1$$下，求$$f(x,y,z)=8xyz​$$的最大值。

  首先定义拉格朗日函数$$F(x)​$$：
  $$
  F(x,\lambda )=f(x)+\sum_{k=1}^{l}\lambda _{k}h_{k}(x)
  $$
  其中$$\lambda _{k}$$是各个约束条件的待定系数。

  然后求解变量的偏导方程：
  $$
  \frac{\partial F}{\partial x_{i}}=0 \ \ .... \ \ \frac{\partial F}{\partial \lambda_{k}}=0 
  $$
  如果有$$l$$个约束条件，就应该有$$l+1$$个方程。求出的方程组的解就可能是最优化值（高等数学中提到的极值），将结果带回原方程验证就可得到解。

  回到上面的题目，通过拉格朗日乘子法将问题转化为
  $$
  \begin{aligned} 
  F(x,y,z,\lambda )&=f(x,y,z)+\lambda \varphi (x,y,z)\\
  &=8xyz+\lambda (\frac{x^{2}}{a^{2}}+\frac{y^{2}}{b^{2}}+\frac{z^{2}}{c^{2}}-1)
  \end{aligned}
  $$
  对$$F(x,y,z,\lambda )$$求偏导得到
  $$
  \begin{aligned} 
  \frac{\partial F(x,y,z,\lambda )}{\partial x} = 8yz+\frac{2\lambda x}{a^{2}}=0\\
  \frac{\partial F(x,y,z,\lambda )}{\partial y} = 8xz+\frac{2\lambda y}{b^{2}}=0\\
  \frac{\partial F(x,y,z,\lambda )}{\partial z} = 8xy+\frac{2\lambda z}{c^{2}}=0\\
  \frac{\partial F(x,y,z,\lambda )}{\partial \lambda } =\frac{x^{2}}{a^{2}}+\frac{y^{2}}{b^{2}}+\frac{z^{2}}{c^{2}}-1 = 0
  \end{aligned} 
  $$
  联立前面三个方程得到$$bx = ay$$和$$az = cx$$，带入第四个方程解之
  $$
  x=\frac{\sqrt{3}}{3}a \ \ \ y=\frac{\sqrt{3}}{3}b \ \ \ z=\frac{\sqrt{3}}{3}c
  $$
  带入解得最大体积为：
  $$
  V_{max}=\frac{8\sqrt{3}}{9}abc
  $$

- 不等式约束条件

设目标函数$$f(x)$$,不等式约束为$$g(x)$$，等式约束$$h(x)$$。此时的约束优化问题描述如下：
$$
\begin{aligned} 
min \ \ \ &f(x) \\
s.t. \ \ &h_{j}(x)=0 \ \ \ j=1,2,...,p \\
&g_{k}(x)\leq 0\ \ \  k = 1,2,...,q
\end{aligned}
$$
则我们定义不等式约束下的拉格朗日函数$$L$$,则$$L$$表达式为：
$$
L(x,\lambda ,\mu )=f(x)+\sum_{j=1}^{p}\lambda _{j}h_{j}(x)+\sum_{k=1}^{q}\mu _{k}g_{k}(x)
$$
其中$$f(x)$$是原目标函数，$$h_{j}(x)$$是第$$j$$个等式约束条件，$$\lambda _{j}$$是对应的约束系数，$$g_{k}(x)$$是不等式约束，$$\mu _{k}$$是对应的约束系数。

此时若要求解上述优化问题，必须满足下述条件：
$$
\begin{aligned} 
&\frac{\partial L}{\partial x}\mid _{x=x^{\ast }}=0 \\ 
&\lambda _{j}\neq 0 \\
&\mu _{k}\geqslant 0 \\
&\mu _{k}g_{k}(x^{\ast })=0 \\
&h_{j}(x^{\ast })=0 \\
&g_{k}(x^{\ast })\leq 0
\end{aligned}
$$
这些求解条件就是**KKT条件**。

### 回归

## 无监督学习  

### 聚类 

## 模型评估与选择

### 经验误差与过拟合

**错误率**：分类错误的样本数占样本总数的比例。

**精度**：1-错误率

**误差**：学习器的实际预测输出与样本的真实输出直接的差异。

**训练误差**：学习器在训练集上的误差。

**泛化误差**：学习器在新样本上的误差。

**过拟合**：学习器把训练样本学得“太好”了的时候，很可能已经把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质，这样就会导致泛化性能下降。

**欠拟合**：对训练样本的一般性质尚未学好。

### 评估方法  

通过实验测试对学习器的泛化误差进行评估并进而做出选择。

##### 留出法

直接将数据集划分为两个互斥的集合，其中一个集合作为训练集，另一个集合作为测试集。

##### 交叉验证法  

将数据集划分为k个大小相似的互斥子集，然后每次用k-1个子集作为训练集，剩下的子集作为测试集。

###### k折交叉

from sklearn.datasets import load_iris
from sklearn.neighbors import KNeighborsClassifier

import warnings

warnings.filterwarnings("ignore")

from sklearn.model_selection import cross_val_score
data = load_iris()
train = data.data
test = data.target

knn = KNeighborsClassifier(5)
scores = cross_val_score(knn,train,test,cv=10,scoring='accuracy')

**StratifiedKFold** 是 k-fold 的变种，会返回 stratified（分层） 的折叠：每个小集合中， 各个类别的样例比例大致和完整数据集中相同

###### 留一法

留一法不受随机样本划分方式的影响，因为m个样本只有唯一的方式划分为m个子集——每个子集包含一个样本；留一法使用的训练集与初始数据集相比只少了一个样本，这就使得在大多数情况下，留一法中被实际评估的模型与期望评估的用D训练出的模型很相似。留一法的评估结果往往被认为比较准确。然而，留一法在数据集比较大时，比如数据集包含1百万个样本，则需训练1百万个模型。

##### 自助法

自助法（Bootstrap Method，Bootstrapping或自助抽样法）是一种从给定训练集中有放回的均匀抽样，也就是说，每当选中一个样本，它等可能地被再次选中并被再次添加到训练集中。

![](D:\study resource\code\machine_learning\image\自助法.png)

#### 性能度量  

回归任务最常用的性能度量是均方误差。

**错误率**：分类错误的样本数占样本总数的比例。

**精度  **：分类正确的样本数占样本总数的比例。

分类结果混淆矩阵

<table>
    <tr>
        <td>真实情况</td>
        <td>预测结果</td>
        <td>预测结果</td>
    </tr>
    <tr>
        <td></td>
        <td>正例</td>
        <td>反例</td>
    </tr>
    <tr>
        <td>正例</td>
        <td>TP(真正例)</td>
        <td>FN（假反例）</td>
    </tr>
    <tr>
        <td>反例</td>
        <td>FP(假正例)</td>
        <td>TN(真反例)</td>
    </tr>
</table>
**查准率**:预测为正的样本中，真正例占的比例。P = TP/(TP+ FP)

**查全率**：正例有多少被预测正确。P=TP/(TP+FN)

## xgboost



## 常见问题

### 过拟合解决办法

- 正则化

- 剪枝处理：剪枝是决策树中一种控制过拟合的方法。主要有预剪枝和后剪枝两种。预剪枝通过在训练过程中控制树深、叶子节点数、叶子节点中样本的个数来控制树的复杂度。后剪枝则是在训练好树模型之后，采用交叉验证的方式进行剪枝以找到最优的树模型。

- 提前终止迭代（Early stopping）：该方法主要是用在神经网络中的，在神经网络的训练过程中我们会初始化一组较小的权值参数，此时模型的拟合能力较弱，通过迭代训练来提高模型的拟合能力，随着迭代次数的增大，部分的权值也会不断的增大。如果我们提前终止迭代可以有效的控制权值参数的大小，从而降低模型的复杂度。

- 权值共享：权值共享最常见的就是在卷积神经网络中，权值共享的目的旨在减小模型中的参数，同时还能较少计算量。在循环神经网络中也用到了权值共享。

- 增加噪声：这也是深度学习中的一种避免过拟合的方法（没办法，深度学习模型太复杂，容易过拟合），添加噪声的途径有很多，可以在输入数据上添加，增大数据的多样性，可以在权值上添加噪声，这种方法类似于L2正则化。

- batch normalization:BM算法是一种非常有用的正则化方法，而且可以让大型的卷积神经网络快速收敛，同时还能提高分类的准确率，而且可以不需要使用局部响应归一化处理，也可以不需要加入Dropout。BM算法会将每一层的输入值做归一化处理，并且会重构归一化处理之后的数据，确保数据的分布不会发生变化。

- bagging和boosting：Bagging和Boosting是机器学习中的集成方法，多个模型的组合可以弱化每个模型中的异常点的影响，保留模型之间的通性，弱化单个模型的特性。

- dropout:Dropout是深度学习中最常用的控制过拟合的方法，主要用在全连接层处。Dropout方法是在一定的概率上（通常设置为0.5，原因是此时随机生成的网络结构最多）隐式的去除网络中的神经元。

- 增加训练数据：发生过拟合最常见的现象就是数据量太少而模型太复杂。**可以利用现有数据进行扩充，例如在图像识别中，如果没有足够的图片训练，可以把已有的图片进行旋转、拉伸、镜像、对称等**。

- 使用early_stopping参数：在xgboost模型和神经网络模型中，可以设定此参数在验证集上的准确率或者其它评判标准在多少次迭代中不再变化，则停止训练。


### 欠拟合解决办法  

- 模型复杂化
- 增加更多的特征，使输入数据具有更强的表达力
- 调整参数和超参数
- 增加训练数据往往没用：欠拟合本来就是模型的学习能力不足，增加再多的数据也没能力拟合好
- 降低正则化约束

### bagging、boosting、随机森林



**boosting**是一个迭代的过程，用于自适应地改变训练样本的分布，使得基分类器聚焦在那些很难分的样本上。boosting会给每个训练样本赋予一个权值，而且可以再每轮提升过程结束时自动地调整权值。开始时，所有的样本都赋予相同的权值1/N，从而使得它们被选作训练的可能性都一样。根据训练样本的抽样分布来抽取样本，得到新的样本集。然后，由该训练集归纳一个分类器，并用它对原数据集中的所有样本进行分类。每轮提升结束时，更新训练集样本的权值。增加被错误分类的样本的权值，减小被正确分类的样本的权值，这使得分类器在随后的迭代中关注那些很难分类的样本。

**Bagging**又叫自助聚集，是一种根据均匀概率分布从数据中重复抽样（有放回）的技术。每个抽样生成的自助样本集上，训练一个基分类器；对训练过的分类器进行投票，将测试样本指派到得票最高的类中。每个自助样本集都和原数据一样大，有放回抽样，一些样本可能在同一训练集中出现多次，一些可能被忽略。

**随机森林**以决策树为基学习器构建Bagging集成(或者boosting集成)，进一步在决策树的训练过程中引入随机属性选择。传统决策树在选择划分属性的时候是在当前节点所有的属性集合中选出一个左右属性进行划分；而在RF中，对基决策树的每个节点，先从该节点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最优属性用于划分。这里的参数k控制了随机性的引入程度。如果k=d（全部属性集），则基决策树的构建=传统决策树构建。如果k=1，基决策树每个节点随机选择一个属性进行划分。一般推荐k=log2d。

### 没有免费午餐定理

没有一种机器学习算法是适用所有情况的。更一般地说，对于所有机器学习问题，任何一种算法（包括瞎猜）的期望结果都是一样的。

若学习算法a在某些问题上比学习算法b要好，那么必然存在另一些问题，在这些问题中b比a表现更好。



### XGBoost如何处理缺失值

- 在寻找split point的时候，不会对该特征为missing的样本进行遍历统计，只对该列特征值为non-missing的样本上对应的特征值进行遍历，通过这个技巧来减少了为稀疏离散特征寻找split point的时间开销。
- 在逻辑实现上，为了保证完备性，会分别处理将missing该特征值的样本分配到左叶子结点和右叶子结点的两种情形，计算增益后选择增益大的方向进行分裂即可。
- 如果在训练中没有缺失值而在预测中出现缺失，那么会自动将缺失值的划分方向放到右子树